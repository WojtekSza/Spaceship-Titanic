{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.8",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "name": "Predicting Transported in train",
    "modifiedBy": "admin"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Predicting Transported in train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Notebook automatically generated from your model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model Random forest, trained on 2023-06-11 14:46:41."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Generated on 2023-06-11 14:58:18.495886"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "prediction\nThis notebook will reproduce the steps for a BINARY_CLASSIFICATION on  train.\nThe main objective is to predict the variable Transported"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Warning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The goal of this notebook is to provide an easily readable and explainable code that reproduces the main steps\nof training the model. It is not complete: some of the preprocessing done by the DSS visual machine learning is not\nreplicated in this notebook. This notebook will not give the same results and model performance as the DSS visual machine\nlearning model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\u0027s start with importing the required libs :"
      ]
    },
    {
      "execution_count": 1,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import sys\nimport dataiku\nimport numpy as np\nimport pandas as pd\nimport sklearn as sk\nimport dataiku.core.pandasutils as pdu\nfrom dataiku.doctor.preprocessing import PCA\nfrom collections import defaultdict, Counter"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And tune pandas display options:"
      ]
    },
    {
      "execution_count": 2,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pd.set_option(\u0027display.width\u0027, 3000)\npd.set_option(\u0027display.max_rows\u0027, 200)\npd.set_option(\u0027display.max_columns\u0027, 200)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Importing base data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first step is to get our machine learning dataset:"
      ]
    },
    {
      "execution_count": 3,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# We apply the preparation that you defined. You should not modify this.\npreparation_steps \u003d []\npreparation_output_schema \u003d {\u0027columns\u0027: [{\u0027name\u0027: \u0027PassengerId\u0027, \u0027type\u0027: \u0027string\u0027}, {\u0027name\u0027: \u0027HomePlanet\u0027, \u0027type\u0027: \u0027string\u0027}, {\u0027name\u0027: \u0027CryoSleep\u0027, \u0027type\u0027: \u0027boolean\u0027}, {\u0027name\u0027: \u0027Cabin\u0027, \u0027type\u0027: \u0027string\u0027}, {\u0027name\u0027: \u0027Destination\u0027, \u0027type\u0027: \u0027string\u0027}, {\u0027name\u0027: \u0027Age\u0027, \u0027type\u0027: \u0027double\u0027}, {\u0027name\u0027: \u0027VIP\u0027, \u0027type\u0027: \u0027boolean\u0027}, {\u0027name\u0027: \u0027RoomService\u0027, \u0027type\u0027: \u0027double\u0027}, {\u0027name\u0027: \u0027FoodCourt\u0027, \u0027type\u0027: \u0027double\u0027}, {\u0027name\u0027: \u0027ShoppingMall\u0027, \u0027type\u0027: \u0027double\u0027}, {\u0027name\u0027: \u0027Spa\u0027, \u0027type\u0027: \u0027double\u0027}, {\u0027name\u0027: \u0027VRDeck\u0027, \u0027type\u0027: \u0027double\u0027}, {\u0027name\u0027: \u0027Name\u0027, \u0027type\u0027: \u0027string\u0027}, {\u0027name\u0027: \u0027Transported\u0027, \u0027type\u0027: \u0027boolean\u0027}], \u0027userModified\u0027: False}\n\nml_dataset_handle \u003d dataiku.Dataset(\u0027train\u0027)\nml_dataset_handle.set_preparation_steps(preparation_steps, preparation_output_schema)\n%time ml_dataset \u003d ml_dataset_handle.get_dataframe(limit \u003d 100000)\n\nprint (\u0027Base data has %i rows and %i columns\u0027 % (ml_dataset.shape[0], ml_dataset.shape[1]))\n# Five first records\",\nml_dataset.head(5)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": "CPU times: user 24.8 ms, sys: 50 µs, total: 24.9 ms\nWall time: 223 ms\nBase data has 8693 rows and 14 columns\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "  PassengerId HomePlanet CryoSleep  Cabin  Destination   Age    VIP  RoomService  FoodCourt  ShoppingMall     Spa  VRDeck               Name  Transported\n0     0001_01     Europa     False  B/0/P  TRAPPIST-1e  39.0  False          0.0        0.0           0.0     0.0     0.0    Maham Ofracculy        False\n1     0002_01      Earth     False  F/0/S  TRAPPIST-1e  24.0  False        109.0        9.0          25.0   549.0    44.0       Juanna Vines         True\n2     0003_01     Europa     False  A/0/S  TRAPPIST-1e  58.0   True         43.0     3576.0           0.0  6715.0    49.0      Altark Susent        False\n3     0003_02     Europa     False  A/0/S  TRAPPIST-1e  33.0  False          0.0     1283.0         371.0  3329.0   193.0       Solam Susent        False\n4     0004_01      Earth     False  F/1/S  TRAPPIST-1e  16.0  False        303.0       70.0         151.0   565.0     2.0  Willy Santantines         True",
            "text/html": "\n            \u003cbutton style\u003d\"display:none\" \n            class\u003d\"btn btn-default ipython-export-btn\" \n            id\u003d\"btn-df-96d33cfc-f330-4238-9794-0de5c83a8ece\" \n            onclick\u003d\"_export_df(\u002796d33cfc-f330-4238-9794-0de5c83a8ece\u0027)\"\u003e\n                Export dataframe\n            \u003c/button\u003e\n            \n            \u003cscript\u003e\n                \n                function _check_export_df_possible(dfid,yes_fn,no_fn) {\n                    console.log(\u0027Checking dataframe exportability...\u0027)\n                    if(!IPython || !IPython.notebook || !IPython.notebook.kernel || !IPython.notebook.kernel) {\n                        console.log(\u0027Export is not possible (IPython kernel is not available)\u0027)\n                        if(no_fn) {\n                            no_fn();\n                        }\n                    } else {\n                        var pythonCode \u003d \u0027from dataiku.notebook.export import IPythonExporter;IPythonExporter._check_export_stdout(\"\u0027+dfid+\u0027\")\u0027;\n                        IPython.notebook.kernel.execute(pythonCode,{iopub: {output: function(resp) {\n                            console.info(\"Exportability response\", resp);\n                            var size \u003d /^([0-9]+)x([0-9]+)$/.exec(resp.content.data || resp.content.text)\n                            if(!size) {\n                                console.log(\u0027Export is not possible (dataframe is not in-memory anymore)\u0027)\n                                if(no_fn) {\n                                    no_fn();\n                                }\n                            } else {\n                                console.log(\u0027Export is possible\u0027)\n                                if(yes_fn) {\n                                    yes_fn(1*size[1],1*size[2]);\n                                }\n                            }\n                        }}});\n                    }\n                }\n            \n                function _export_df(dfid) {\n                    \n                    var btn \u003d $(\u0027#btn-df-\u0027+dfid);\n                    var btns \u003d $(\u0027.ipython-export-btn\u0027);\n                    \n                    _check_export_df_possible(dfid,function() {\n                        \n                        window.parent.openExportModalFromIPython(\u0027Pandas dataframe\u0027,function(data) {\n                            btns.prop(\u0027disabled\u0027,true);\n                            btn.text(\u0027Exporting...\u0027);\n                            var command \u003d \u0027from dataiku.notebook.export import IPythonExporter;IPythonExporter._run_export(\"\u0027+dfid+\u0027\",\"\u0027+data.exportId+\u0027\")\u0027;\n                            var callback \u003d {iopub:{output: function(resp) {\n                                console.info(\"CB resp:\", resp);\n                                _check_export_df_possible(dfid,function(rows, cols) {\n                                    $(\u0027#btn-df-\u0027+dfid)\n                                        .css(\u0027display\u0027,\u0027inline-block\u0027)\n                                        .text(\u0027Export this dataframe (\u0027+rows+\u0027 rows, \u0027+cols+\u0027 cols)\u0027)\n                                        .prop(\u0027disabled\u0027,false);\n                                },function() {\n                                    $(\u0027#btn-df-\u0027+dfid).css(\u0027display\u0027,\u0027none\u0027);\n                                });\n                            }}};\n                            IPython.notebook.kernel.execute(command,callback,{silent:false}); // yes, silent now defaults to true. figures.\n                        });\n                    \n                    }, function(){\n                            alert(\u0027Unable to export : the Dataframe object is not loaded in memory\u0027);\n                            btn.css(\u0027display\u0027,\u0027none\u0027);\n                    });\n                    \n                }\n                \n                (function(dfid) {\n                \n                    var retryCount \u003d 10;\n                \n                    function is_valid_websock(s) {\n                        return s \u0026\u0026 s.readyState\u003d\u003d1;\n                    }\n                \n                    function check_conn() {\n                        \n                        if(!IPython || !IPython.notebook) {\n                            // Don\u0027t even try to go further\n                            return;\n                        }\n                        \n                        // Check if IPython is ready\n                        console.info(\"Checking conn ...\")\n                        if(IPython.notebook.kernel\n                        \u0026\u0026 IPython.notebook.kernel\n                        \u0026\u0026 is_valid_websock(IPython.notebook.kernel.ws)\n                        ) {\n                            \n                            _check_export_df_possible(dfid,function(rows, cols) {\n                                $(\u0027#btn-df-\u0027+dfid).css(\u0027display\u0027,\u0027inline-block\u0027);\n                                $(\u0027#btn-df-\u0027+dfid).text(\u0027Export this dataframe (\u0027+rows+\u0027 rows, \u0027+cols+\u0027 cols)\u0027);\n                            });\n                            \n                        } else {\n                            console.info(\"Conditions are not ok\", IPython.notebook.kernel);\n                            \n                            // Retry later\n                            \n                            if(retryCount\u003e0) {\n                                setTimeout(check_conn,500);\n                                retryCount--;\n                            }\n                            \n                        }\n                    };\n                    \n                    setTimeout(check_conn,100);\n                    \n                })(\"96d33cfc-f330-4238-9794-0de5c83a8ece\");\n                \n            \u003c/script\u003e\n            \n        \u003cdiv\u003e\n\u003cstyle scoped\u003e\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\u003c/style\u003e\n\u003ctable border\u003d\"1\" class\u003d\"dataframe\"\u003e\n  \u003cthead\u003e\n    \u003ctr style\u003d\"text-align: right;\"\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003ePassengerId\u003c/th\u003e\n      \u003cth\u003eHomePlanet\u003c/th\u003e\n      \u003cth\u003eCryoSleep\u003c/th\u003e\n      \u003cth\u003eCabin\u003c/th\u003e\n      \u003cth\u003eDestination\u003c/th\u003e\n      \u003cth\u003eAge\u003c/th\u003e\n      \u003cth\u003eVIP\u003c/th\u003e\n      \u003cth\u003eRoomService\u003c/th\u003e\n      \u003cth\u003eFoodCourt\u003c/th\u003e\n      \u003cth\u003eShoppingMall\u003c/th\u003e\n      \u003cth\u003eSpa\u003c/th\u003e\n      \u003cth\u003eVRDeck\u003c/th\u003e\n      \u003cth\u003eName\u003c/th\u003e\n      \u003cth\u003eTransported\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003cth\u003e0\u003c/th\u003e\n      \u003ctd\u003e0001_01\u003c/td\u003e\n      \u003ctd\u003eEuropa\u003c/td\u003e\n      \u003ctd\u003eFalse\u003c/td\u003e\n      \u003ctd\u003eB/0/P\u003c/td\u003e\n      \u003ctd\u003eTRAPPIST-1e\u003c/td\u003e\n      \u003ctd\u003e39.0\u003c/td\u003e\n      \u003ctd\u003eFalse\u003c/td\u003e\n      \u003ctd\u003e0.0\u003c/td\u003e\n      \u003ctd\u003e0.0\u003c/td\u003e\n      \u003ctd\u003e0.0\u003c/td\u003e\n      \u003ctd\u003e0.0\u003c/td\u003e\n      \u003ctd\u003e0.0\u003c/td\u003e\n      \u003ctd\u003eMaham Ofracculy\u003c/td\u003e\n      \u003ctd\u003eFalse\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e1\u003c/th\u003e\n      \u003ctd\u003e0002_01\u003c/td\u003e\n      \u003ctd\u003eEarth\u003c/td\u003e\n      \u003ctd\u003eFalse\u003c/td\u003e\n      \u003ctd\u003eF/0/S\u003c/td\u003e\n      \u003ctd\u003eTRAPPIST-1e\u003c/td\u003e\n      \u003ctd\u003e24.0\u003c/td\u003e\n      \u003ctd\u003eFalse\u003c/td\u003e\n      \u003ctd\u003e109.0\u003c/td\u003e\n      \u003ctd\u003e9.0\u003c/td\u003e\n      \u003ctd\u003e25.0\u003c/td\u003e\n      \u003ctd\u003e549.0\u003c/td\u003e\n      \u003ctd\u003e44.0\u003c/td\u003e\n      \u003ctd\u003eJuanna Vines\u003c/td\u003e\n      \u003ctd\u003eTrue\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e2\u003c/th\u003e\n      \u003ctd\u003e0003_01\u003c/td\u003e\n      \u003ctd\u003eEuropa\u003c/td\u003e\n      \u003ctd\u003eFalse\u003c/td\u003e\n      \u003ctd\u003eA/0/S\u003c/td\u003e\n      \u003ctd\u003eTRAPPIST-1e\u003c/td\u003e\n      \u003ctd\u003e58.0\u003c/td\u003e\n      \u003ctd\u003eTrue\u003c/td\u003e\n      \u003ctd\u003e43.0\u003c/td\u003e\n      \u003ctd\u003e3576.0\u003c/td\u003e\n      \u003ctd\u003e0.0\u003c/td\u003e\n      \u003ctd\u003e6715.0\u003c/td\u003e\n      \u003ctd\u003e49.0\u003c/td\u003e\n      \u003ctd\u003eAltark Susent\u003c/td\u003e\n      \u003ctd\u003eFalse\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e3\u003c/th\u003e\n      \u003ctd\u003e0003_02\u003c/td\u003e\n      \u003ctd\u003eEuropa\u003c/td\u003e\n      \u003ctd\u003eFalse\u003c/td\u003e\n      \u003ctd\u003eA/0/S\u003c/td\u003e\n      \u003ctd\u003eTRAPPIST-1e\u003c/td\u003e\n      \u003ctd\u003e33.0\u003c/td\u003e\n      \u003ctd\u003eFalse\u003c/td\u003e\n      \u003ctd\u003e0.0\u003c/td\u003e\n      \u003ctd\u003e1283.0\u003c/td\u003e\n      \u003ctd\u003e371.0\u003c/td\u003e\n      \u003ctd\u003e3329.0\u003c/td\u003e\n      \u003ctd\u003e193.0\u003c/td\u003e\n      \u003ctd\u003eSolam Susent\u003c/td\u003e\n      \u003ctd\u003eFalse\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e4\u003c/th\u003e\n      \u003ctd\u003e0004_01\u003c/td\u003e\n      \u003ctd\u003eEarth\u003c/td\u003e\n      \u003ctd\u003eFalse\u003c/td\u003e\n      \u003ctd\u003eF/1/S\u003c/td\u003e\n      \u003ctd\u003eTRAPPIST-1e\u003c/td\u003e\n      \u003ctd\u003e16.0\u003c/td\u003e\n      \u003ctd\u003eFalse\u003c/td\u003e\n      \u003ctd\u003e303.0\u003c/td\u003e\n      \u003ctd\u003e70.0\u003c/td\u003e\n      \u003ctd\u003e151.0\u003c/td\u003e\n      \u003ctd\u003e565.0\u003c/td\u003e\n      \u003ctd\u003e2.0\u003c/td\u003e\n      \u003ctd\u003eWilly Santantines\u003c/td\u003e\n      \u003ctd\u003eTrue\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003c/div\u003e"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Initial data management"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The preprocessing aims at making the dataset compatible with modeling.\nAt the end of this step, we will have a matrix of float numbers, with no missing values.\nWe\u0027ll use the features and the preprocessing steps defined in Models.\n\nLet\u0027s only keep selected features"
      ]
    },
    {
      "execution_count": 4,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ml_dataset \u003d ml_dataset[[\u0027Destination\u0027, \u0027VRDeck\u0027, \u0027Transported\u0027, \u0027Spa\u0027, \u0027Name\u0027, \u0027Cabin\u0027, \u0027FoodCourt\u0027, \u0027HomePlanet\u0027, \u0027RoomService\u0027, \u0027VIP\u0027, \u0027ShoppingMall\u0027, \u0027CryoSleep\u0027, \u0027Age\u0027]]"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\u0027s first coerce categorical columns into unicode, numerical features into floats."
      ]
    },
    {
      "execution_count": 5,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# astype(\u0027unicode\u0027) does not work as expected\n\ndef coerce_to_unicode(x):\n    if sys.version_info \u003c (3, 0):\n        if isinstance(x, str):\n            return unicode(x,\u0027utf-8\u0027)\n        else:\n            return unicode(x)\n    else:\n        return str(x)\n\n\ncategorical_features \u003d [\u0027Destination\u0027, \u0027Name\u0027, \u0027Cabin\u0027, \u0027HomePlanet\u0027, \u0027VIP\u0027, \u0027CryoSleep\u0027]\nnumerical_features \u003d [\u0027VRDeck\u0027, \u0027Spa\u0027, \u0027FoodCourt\u0027, \u0027RoomService\u0027, \u0027ShoppingMall\u0027, \u0027Age\u0027]\ntext_features \u003d []\nfrom dataiku.doctor.utils import datetime_to_epoch\nfor feature in categorical_features:\n    ml_dataset[feature] \u003d ml_dataset[feature].apply(coerce_to_unicode)\nfor feature in text_features:\n    ml_dataset[feature] \u003d ml_dataset[feature].apply(coerce_to_unicode)\nfor feature in numerical_features:\n    if ml_dataset[feature].dtype \u003d\u003d np.dtype(\u0027M8[ns]\u0027) or (hasattr(ml_dataset[feature].dtype, \u0027base\u0027) and ml_dataset[feature].dtype.base \u003d\u003d np.dtype(\u0027M8[ns]\u0027)):\n        ml_dataset[feature] \u003d datetime_to_epoch(ml_dataset[feature])\n    else:\n        ml_dataset[feature] \u003d ml_dataset[feature].astype(\u0027double\u0027)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are now going to handle the target variable and store it in a new variable:"
      ]
    },
    {
      "execution_count": 6,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "target_map \u003d {\u0027False\u0027: 0, \u0027True\u0027: 1}\nml_dataset[\u0027__target__\u0027] \u003d ml_dataset[\u0027Transported\u0027].map(str).map(target_map)\ndel ml_dataset[\u0027Transported\u0027]\n\n\n# Remove rows for which the target is unknown.\nml_dataset \u003d ml_dataset[~ml_dataset[\u0027__target__\u0027].isnull()]"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Cross-validation strategy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dataset needs to be split into 2 new sets, one that will be used for training the model (train set)\nand another that will be used to test its generalization capability (test set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is a simple cross-validation strategy."
      ]
    },
    {
      "execution_count": 7,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "train, test \u003d pdu.split_train_valid(ml_dataset, prop\u003d0.8)\nprint (\u0027Train data has %i rows and %i columns\u0027 % (train.shape[0], train.shape[1]))\nprint (\u0027Test data has %i rows and %i columns\u0027 % (test.shape[0], test.shape[1]))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": "Train data has 6954 rows and 13 columns\nTest data has 1739 rows and 13 columns\n",
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Features preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first thing to do at the features level is to handle the missing values.\nLet\u0027s reuse the settings defined in the model"
      ]
    },
    {
      "execution_count": 8,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "drop_rows_when_missing \u003d []\nimpute_when_missing \u003d [{\u0027feature\u0027: \u0027VRDeck\u0027, \u0027impute_with\u0027: \u0027MEAN\u0027}, {\u0027feature\u0027: \u0027Spa\u0027, \u0027impute_with\u0027: \u0027MEAN\u0027}, {\u0027feature\u0027: \u0027FoodCourt\u0027, \u0027impute_with\u0027: \u0027MEAN\u0027}, {\u0027feature\u0027: \u0027RoomService\u0027, \u0027impute_with\u0027: \u0027MEAN\u0027}, {\u0027feature\u0027: \u0027ShoppingMall\u0027, \u0027impute_with\u0027: \u0027MEAN\u0027}, {\u0027feature\u0027: \u0027Age\u0027, \u0027impute_with\u0027: \u0027MEAN\u0027}]\n\n# Features for which we drop rows with missing values\"\nfor feature in drop_rows_when_missing:\n    train \u003d train[train[feature].notnull()]\n    test \u003d test[test[feature].notnull()]\n    print (\u0027Dropped missing records in %s\u0027 % feature)\n\n# Features for which we impute missing values\"\nfor feature in impute_when_missing:\n    if feature[\u0027impute_with\u0027] \u003d\u003d \u0027MEAN\u0027:\n        v \u003d train[feature[\u0027feature\u0027]].mean()\n    elif feature[\u0027impute_with\u0027] \u003d\u003d \u0027MEDIAN\u0027:\n        v \u003d train[feature[\u0027feature\u0027]].median()\n    elif feature[\u0027impute_with\u0027] \u003d\u003d \u0027CREATE_CATEGORY\u0027:\n        v \u003d \u0027NULL_CATEGORY\u0027\n    elif feature[\u0027impute_with\u0027] \u003d\u003d \u0027MODE\u0027:\n        v \u003d train[feature[\u0027feature\u0027]].value_counts().index[0]\n    elif feature[\u0027impute_with\u0027] \u003d\u003d \u0027CONSTANT\u0027:\n        v \u003d feature[\u0027value\u0027]\n    train[feature[\u0027feature\u0027]] \u003d train[feature[\u0027feature\u0027]].fillna(v)\n    test[feature[\u0027feature\u0027]] \u003d test[feature[\u0027feature\u0027]].fillna(v)\n    print (\u0027Imputed missing values in feature %s with value %s\u0027 % (feature[\u0027feature\u0027], coerce_to_unicode(v)))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": "Imputed missing values in feature VRDeck with value 297.86727486756917\nImputed missing values in feature Spa with value 312.2928550426345\nImputed missing values in feature FoodCourt with value 450.57533441128913\nImputed missing values in feature RoomService with value 224.24581005586592\nImputed missing values in feature ShoppingMall with value 172.39349330192846\nImputed missing values in feature Age with value 28.639524647887324\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "/home/dataiku/dataiku-dss-9.0.1/python36.packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] \u003d value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now handle the categorical features (still using the settings defined in Models):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\u0027s dummy-encode the following features.\nA binary column is created for each of the 100 most frequent values."
      ]
    },
    {
      "execution_count": 9,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "LIMIT_DUMMIES \u003d 100\n\ncategorical_to_dummy_encode \u003d [\u0027Destination\u0027, \u0027Name\u0027, \u0027Cabin\u0027, \u0027HomePlanet\u0027, \u0027VIP\u0027, \u0027CryoSleep\u0027]\n\n# Only keep the top 100 values\ndef select_dummy_values(train, features):\n    dummy_values \u003d {}\n    for feature in categorical_to_dummy_encode:\n        values \u003d [\n            value\n            for (value, _) in Counter(train[feature]).most_common(LIMIT_DUMMIES)\n        ]\n        dummy_values[feature] \u003d values\n    return dummy_values\n\nDUMMY_VALUES \u003d select_dummy_values(train, categorical_to_dummy_encode)\n\ndef dummy_encode_dataframe(df):\n    for (feature, dummy_values) in DUMMY_VALUES.items():\n        for dummy_value in dummy_values:\n            dummy_name \u003d u\u0027%s_value_%s\u0027 % (feature, coerce_to_unicode(dummy_value))\n            df[dummy_name] \u003d (df[feature] \u003d\u003d dummy_value).astype(float)\n        del df[feature]\n        print (\u0027Dummy-encoded feature %s\u0027 % feature)\n\ndummy_encode_dataframe(train)\n\ndummy_encode_dataframe(test)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": "Dummy-encoded feature Destination\nDummy-encoded feature Name\nDummy-encoded feature Cabin\nDummy-encoded feature HomePlanet\nDummy-encoded feature VIP\nDummy-encoded feature CryoSleep\nDummy-encoded feature Destination\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "/home/dataiku/dataiku-dss-9.0.1/python36.packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] \u003d value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "Dummy-encoded feature Name\nDummy-encoded feature Cabin\nDummy-encoded feature HomePlanet\nDummy-encoded feature VIP\nDummy-encoded feature CryoSleep\n",
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\u0027s rescale numerical features"
      ]
    },
    {
      "execution_count": 10,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "rescale_features \u003d {\u0027VRDeck\u0027: \u0027AVGSTD\u0027, \u0027Spa\u0027: \u0027AVGSTD\u0027, \u0027FoodCourt\u0027: \u0027AVGSTD\u0027, \u0027RoomService\u0027: \u0027AVGSTD\u0027, \u0027ShoppingMall\u0027: \u0027AVGSTD\u0027, \u0027Age\u0027: \u0027AVGSTD\u0027}\nfor (feature_name, rescale_method) in rescale_features.items():\n    if rescale_method \u003d\u003d \u0027MINMAX\u0027:\n        _min \u003d train[feature_name].min()\n        _max \u003d train[feature_name].max()\n        scale \u003d _max - _min\n        shift \u003d _min\n    else:\n        shift \u003d train[feature_name].mean()\n        scale \u003d train[feature_name].std()\n    if scale \u003d\u003d 0.:\n        del train[feature_name]\n        del test[feature_name]\n        print (\u0027Feature %s was dropped because it has no variance\u0027 % feature_name)\n    else:\n        print (\u0027Rescaled %s\u0027 % feature_name)\n        train[feature_name] \u003d (train[feature_name] - shift).astype(np.float64) / scale\n        test[feature_name] \u003d (test[feature_name] - shift).astype(np.float64) / scale"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": "Rescaled VRDeck\nRescaled Spa\nRescaled FoodCourt\nRescaled RoomService\nRescaled ShoppingMall\nRescaled Age\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "/home/dataiku/dataiku-dss-9.0.1/python36.packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] \u003d value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before actually creating our model, we need to split the datasets into their features and labels parts:"
      ]
    },
    {
      "execution_count": 11,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "train_X \u003d train.drop(\u0027__target__\u0027, axis\u003d1)\ntest_X \u003d test.drop(\u0027__target__\u0027, axis\u003d1)\n\ntrain_Y \u003d np.array(train[\u0027__target__\u0027])\ntest_Y \u003d np.array(test[\u0027__target__\u0027])"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can finally create our model !"
      ]
    },
    {
      "execution_count": 12,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\nclf \u003d RandomForestClassifier(n_estimators\u003d100,\n    random_state\u003d1337,\n    max_depth\u003d13,\n    min_samples_leaf\u003d1,\n    verbose\u003d2)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "... And train it"
      ]
    },
    {
      "execution_count": 13,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%time clf.fit(train_X, train_Y)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": "[Parallel(n_jobs\u003d1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs\u003d1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "building tree 1 of 100\nbuilding tree 2 of 100\nbuilding tree 3 of 100\nbuilding tree 4 of 100\nbuilding tree 5 of 100\nbuilding tree 6 of 100\nbuilding tree 7 of 100\nbuilding tree 8 of 100\nbuilding tree 9 of 100\nbuilding tree 10 of 100\nbuilding tree 11 of 100\nbuilding tree 12 of 100\nbuilding tree 13 of 100\nbuilding tree 14 of 100\nbuilding tree 15 of 100\nbuilding tree 16 of 100\nbuilding tree 17 of 100\nbuilding tree 18 of 100\nbuilding tree 19 of 100\nbuilding tree 20 of 100\nbuilding tree 21 of 100\nbuilding tree 22 of 100\nbuilding tree 23 of 100\nbuilding tree 24 of 100\nbuilding tree 25 of 100\nbuilding tree 26 of 100\nbuilding tree 27 of 100\nbuilding tree 28 of 100\nbuilding tree 29 of 100\nbuilding tree 30 of 100\nbuilding tree 31 of 100\nbuilding tree 32 of 100\nbuilding tree 33 of 100\nbuilding tree 34 of 100\nbuilding tree 35 of 100\nbuilding tree 36 of 100\nbuilding tree 37 of 100\nbuilding tree 38 of 100\nbuilding tree 39 of 100\nbuilding tree 40 of 100\nbuilding tree 41 of 100\nbuilding tree 42 of 100\nbuilding tree 43 of 100\nbuilding tree 44 of 100\nbuilding tree 45 of 100\nbuilding tree 46 of 100\nbuilding tree 47 of 100\nbuilding tree 48 of 100\nbuilding tree 49 of 100\nbuilding tree 50 of 100\nbuilding tree 51 of 100\nbuilding tree 52 of 100\nbuilding tree 53 of 100\nbuilding tree 54 of 100\nbuilding tree 55 of 100\nbuilding tree 56 of 100\nbuilding tree 57 of 100\nbuilding tree 58 of 100\nbuilding tree 59 of 100\nbuilding tree 60 of 100\nbuilding tree 61 of 100\nbuilding tree 62 of 100\nbuilding tree 63 of 100\nbuilding tree 64 of 100\nbuilding tree 65 of 100\nbuilding tree 66 of 100\nbuilding tree 67 of 100\nbuilding tree 68 of 100\nbuilding tree 69 of 100\nbuilding tree 70 of 100\nbuilding tree 71 of 100\nbuilding tree 72 of 100\nbuilding tree 73 of 100\nbuilding tree 74 of 100\nbuilding tree 75 of 100\nbuilding tree 76 of 100\nbuilding tree 77 of 100\nbuilding tree 78 of 100\nbuilding tree 79 of 100\nbuilding tree 80 of 100\nbuilding tree 81 of 100\nbuilding tree 82 of 100\nbuilding tree 83 of 100\nbuilding tree 84 of 100\nbuilding tree 85 of 100\nbuilding tree 86 of 100\nbuilding tree 87 of 100\nbuilding tree 88 of 100\nbuilding tree 89 of 100\nbuilding tree 90 of 100\nbuilding tree 91 of 100\nbuilding tree 92 of 100\nbuilding tree 93 of 100\nbuilding tree 94 of 100\nbuilding tree 95 of 100\nbuilding tree 96 of 100\nbuilding tree 97 of 100\nbuilding tree 98 of 100\nbuilding tree 99 of 100\nbuilding tree 100 of 100\nCPU times: user 323 ms, sys: 3.93 ms, total: 327 ms\nWall time: 333 ms\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "[Parallel(n_jobs\u003d1)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "execution_count": 13,
          "data": {
            "text/plain": "RandomForestClassifier(bootstrap\u003dTrue, class_weight\u003dNone, criterion\u003d\u0027gini\u0027,\n            max_depth\u003d13, max_features\u003d\u0027auto\u0027, max_leaf_nodes\u003dNone,\n            min_impurity_decrease\u003d0.0, min_impurity_split\u003dNone,\n            min_samples_leaf\u003d1, min_samples_split\u003d2,\n            min_weight_fraction_leaf\u003d0.0, n_estimators\u003d100, n_jobs\u003dNone,\n            oob_score\u003dFalse, random_state\u003d1337, verbose\u003d2,\n            warm_start\u003dFalse)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Build up our result dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model is now trained, we can apply it to our test set:"
      ]
    },
    {
      "execution_count": 14,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%time _predictions \u003d clf.predict(test_X)\n%time _probas \u003d clf.predict_proba(test_X)\npredictions \u003d pd.Series(data\u003d_predictions, index\u003dtest_X.index, name\u003d\u0027predicted_value\u0027)\ncols \u003d [\n    u\u0027probability_of_value_%s\u0027 % label\n    for (_, label) in sorted([(int(target_map[label]), label) for label in target_map])\n]\nprobabilities \u003d pd.DataFrame(data\u003d_probas, index\u003dtest_X.index, columns\u003dcols)\n\n# Build scored dataset\nresults_test \u003d test_X.join(predictions, how\u003d\u0027left\u0027)\nresults_test \u003d results_test.join(probabilities, how\u003d\u0027left\u0027)\nresults_test \u003d results_test.join(test[\u0027__target__\u0027], how\u003d\u0027left\u0027)\nresults_test \u003d results_test.rename(columns\u003d {\u0027__target__\u0027: \u0027Transported\u0027})"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": "CPU times: user 23 ms, sys: 72 µs, total: 23.1 ms\nWall time: 25.4 ms\nCPU times: user 15.6 ms, sys: 0 ns, total: 15.6 ms\nWall time: 15.6 ms\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "[Parallel(n_jobs\u003d1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs\u003d1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n[Parallel(n_jobs\u003d1)]: Done 100 out of 100 | elapsed:    0.0s finished\n[Parallel(n_jobs\u003d1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs\u003d1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n[Parallel(n_jobs\u003d1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\u0027s have a look at feature importances"
      ]
    },
    {
      "execution_count": 15,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "feature_importances_data \u003d []\nfeatures \u003d train_X.columns\nfor feature_name, feature_importance in zip(features, clf.feature_importances_):\n    feature_importances_data.append({\n        \u0027feature\u0027: feature_name,\n        \u0027importance\u0027: feature_importance\n    })\n\n# Plot the results\npd.DataFrame(feature_importances_data)\\\n    .set_index(\u0027feature\u0027)\\\n    .sort_values(by\u003d\u0027importance\u0027)[-10::]\\\n    .plot(title\u003d\u0027Top 10 most important variables\u0027,\n          kind\u003d\u0027barh\u0027,\n          figsize\u003d(10, 6),\n          color\u003d\u0027#348ABD\u0027,\n          alpha\u003d0.6,\n          lw\u003d\u00271\u0027,\n          edgecolor\u003d\u0027#348ABD\u0027,\n          grid\u003dFalse,)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 15,
          "data": {
            "text/plain": "\u003cmatplotlib.axes._subplots.AxesSubplot at 0x7f3284ed3eb8\u003e"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can measure the model\u0027s accuracy:"
      ]
    },
    {
      "execution_count": 16,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from dataiku.doctor.utils.metrics import mroc_auc_score\ntest_Y_ser \u003d pd.Series(test_Y)\nprint (\u0027AUC value:\u0027, mroc_auc_score(test_Y_ser, _probas))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": "AUC value: 0.8488672651126121\n",
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also view the predictions directly.\nSince scikit-learn only predicts numericals, the labels have been mapped to 0,1,2 ...\nWe need to \u0027reverse\u0027 the mapping to display the initial labels."
      ]
    },
    {
      "execution_count": 17,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "inv_map \u003d { target_map[label] : label for label in target_map}\npredictions.map(inv_map)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 17,
          "data": {
            "text/plain": "0        True\n5       False\n13      False\n14      False\n15       True\n17      False\n21       True\n23       True\n24      False\n26       True\n28       True\n30      False\n33      False\n48      False\n51       True\n53       True\n60      False\n62      False\n71      False\n75      False\n76       True\n80      False\n84      False\n89      False\n98      False\n99      False\n109      True\n110     False\n121     False\n126     False\n127      True\n133     False\n147     False\n151      True\n160      True\n167      True\n194     False\n202      True\n203     False\n206      True\n215      True\n220     False\n227      True\n231     False\n232     False\n235     False\n240      True\n241     False\n246     False\n249      True\n250      True\n251     False\n254     False\n264      True\n268      True\n275     False\n300      True\n303      True\n307      True\n308      True\n311      True\n312     False\n320      True\n333     False\n336     False\n338     False\n341     False\n344      True\n346      True\n349      True\n350      True\n355     False\n363     False\n369     False\n373     False\n381      True\n387      True\n390      True\n393     False\n398     False\n414      True\n419      True\n424      True\n430      True\n438      True\n439     False\n440      True\n450     False\n463      True\n464     False\n467      True\n472      True\n480     False\n487      True\n494     False\n504      True\n507     False\n512      True\n520     False\n522      True\n        ...  \n8224    False\n8233    False\n8234    False\n8235     True\n8236    False\n8243    False\n8244    False\n8250     True\n8251     True\n8254    False\n8256     True\n8258     True\n8261     True\n8266    False\n8272     True\n8277    False\n8280     True\n8282     True\n8283    False\n8288     True\n8291     True\n8297    False\n8300    False\n8308     True\n8311    False\n8318     True\n8320    False\n8330     True\n8335     True\n8342    False\n8344    False\n8345    False\n8350    False\n8351    False\n8353     True\n8360     True\n8369    False\n8373    False\n8375     True\n8381     True\n8382     True\n8390     True\n8392    False\n8393     True\n8405    False\n8414     True\n8421    False\n8423    False\n8429     True\n8433     True\n8441     True\n8443    False\n8452    False\n8453    False\n8457     True\n8459    False\n8462    False\n8465     True\n8471     True\n8472     True\n8474    False\n8476     True\n8479    False\n8485     True\n8486     True\n8495    False\n8497     True\n8498     True\n8501     True\n8503     True\n8510     True\n8513     True\n8535     True\n8537    False\n8551     True\n8555    False\n8568    False\n8577    False\n8584     True\n8590    False\n8597    False\n8599    False\n8602    False\n8603    False\n8604    False\n8605    False\n8623    False\n8624     True\n8627     True\n8629     True\n8643     True\n8652    False\n8654     True\n8660    False\n8661    False\n8670     True\n8676     True\n8679     True\n8681     True\n8689     True\nName: predicted_value, Length: 1739, dtype: object"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That\u0027s it. It\u0027s now up to you to tune your preprocessing, your algo, and your analysis !\n"
      ]
    }
  ]
}